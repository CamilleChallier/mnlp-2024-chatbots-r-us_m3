{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact match (EM)\n",
    "\n",
    "Count the exact matches for both models. \n",
    "\n",
    "The one that has more exact matches is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the EM results\n",
    "EM_base_model_path = \"model_perf_results/EM_results.json-flan-t5-large\"\n",
    "EM_customized_model_path = \"model_perf_results/large/EM_results.json-flan-t5-large-finetuned_customized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the EM results for the base model\n",
    "with open(EM_base_model_path, \"r\") as f:\n",
    "    EM_base_model = json.load(f)\n",
    "\n",
    "# get the EM results for the customized model\n",
    "with open(EM_customized_model_path, \"r\") as f:\n",
    "    EM_customized_model = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3337\n",
      "3337\n"
     ]
    }
   ],
   "source": [
    "print(len(EM_base_model))\n",
    "print(len(EM_customized_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of EM (so the number of ones)\n",
    "EM_base_model_count = sum(EM_base_model)\n",
    "EM_customized_model_count = sum(EM_customized_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM_base_model_count:  49\n",
      "EM_customized_model_count:  54\n"
     ]
    }
   ],
   "source": [
    "print(\"EM_base_model_count: \", EM_base_model_count)\n",
    "print(\"EM_customized_model_count: \", EM_customized_model_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "\n",
    "Count the mean F1 score over the whole test dataset for both models.\n",
    "\n",
    "The model with the higher average F1 score is the better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the F1 results\n",
    "F1_base_model_path = \"model_perf_results/F1_results.json-flan-t5-large\"\n",
    "F1_customized_model_path = \"model_perf_results/large/F1_results.json-flan-t5-large-finetuned_customized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the F1 results for the base model\n",
    "with open(F1_base_model_path, \"r\") as f:\n",
    "    F1_base_model = json.load(f)\n",
    "\n",
    "# get the F1 results for the customized model\n",
    "with open(F1_customized_model_path, \"r\") as f:\n",
    "    F1_customized_model = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3337\n",
      "3337\n"
     ]
    }
   ],
   "source": [
    "print(len(F1_base_model))\n",
    "print(len(F1_customized_model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average F1 score over the whole test dataset\n",
    "F1_base_model_average = sum(F1_base_model) / len(F1_base_model)\n",
    "F1_customized_model_average = sum(F1_customized_model) / len(F1_customized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_base_model_average:  0.0813727123051764\n",
      "F1_customized_model_average:  0.08771781673344088\n"
     ]
    }
   ],
   "source": [
    "print(\"F1_base_model_average: \", F1_base_model_average)\n",
    "print(\"F1_customized_model_average: \", F1_customized_model_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEURT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the bleurt results\n",
    "bleurt_base_model_path = \"model_perf_results/bleurt_results.json-flan-t5-large\"\n",
    "bleurt_customized_model_path = \"model_perf_results/large/bleurt_results.json-flan-t5-large-non_finetuned_customized\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the bleurt results for the base model\n",
    "with open(bleurt_base_model_path, \"r\") as file:\n",
    "    bleurt_base_model = json.load(file)\n",
    "\n",
    "# get the bleurt results for the customized model\n",
    "with open(bleurt_customized_model_path, \"r\") as file:\n",
    "    bleurt_customized_model = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3337\n",
      "3337\n"
     ]
    }
   ],
   "source": [
    "print(len(bleurt_base_model))\n",
    "print(len(bleurt_customized_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bleurt(bleurt1, bleurt2):\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    equal_score = 0\n",
    "    \n",
    "    for val1, val2 in zip(bleurt1, bleurt2):\n",
    "        if val1 > val2:\n",
    "            score1 += 1\n",
    "        elif val2 > val1:\n",
    "            score2 += 1\n",
    "        else:\n",
    "            equal_score += 1\n",
    "\n",
    "    return score1, score2, equal_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleurt_base_score, bleurt_customized_score, bleurt_equal_score = compare_bleurt(bleurt_base_model, bleurt_customized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleurt_base_score:  759\n",
      "bleurt_customized_score:  1504\n",
      "bleurt_equal_score:  1074\n"
     ]
    }
   ],
   "source": [
    "print(\"bleurt_base_score: \", bleurt_base_score)\n",
    "print(\"bleurt_customized_score: \", bleurt_customized_score)\n",
    "print(\"bleurt_equal_score: \", bleurt_equal_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3337\n"
     ]
    }
   ],
   "source": [
    "print('total', bleurt_base_score + bleurt_customized_score + bleurt_equal_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
